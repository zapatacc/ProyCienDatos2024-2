{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab6d2b4578bbb04",
   "metadata": {},
   "source": [
    "# Seguimiento de Experimentos\n",
    "https://neptune.ai/blog/ml-experiment-tracking\n",
    "\n",
    " <img style=\"display: block; margin: auto;\" src=\"./images/mlops-experiment-tracking-excalidraw.png\" width=\"680\" height=\"50\">\n",
    " \n",
    "## 1. Motivación\n",
    "\n",
    "Imaginemos que se está tratando de desarrollar la receta perfecta para las mejores galletas con chispas de chocolate. \n",
    "- Después del primer intento, se decide aumentar la cantidad de harina. \n",
    "- En otro momento, se agregan más chispas de chocolate. \n",
    "- Luego, se prueba añadiendo nueces. \n",
    "- Al final, se habrán probado una docena de recetas, pero ¿cómo se sabría cuál fue la mejor?\n",
    "\n",
    "Seguramente se estaría de acuerdo en que tomar notas durante este proceso sería una buena idea. Anotar los ingredientes y cómo resultaron las galletas ayudaría a saber qué funcionó mejor.\n",
    "\n",
    "Ahora consideremos la siguiente historia:\n",
    "\n",
    "    ... Hasta ahora, todo se ha estado haciendo de manera manual y algo ad hoc.\n",
    "    \n",
    "    Algunas personas están usando esto, otras personas están usando aquello; está todo desorganizado.\n",
    "    \n",
    "    No tenemos nada estandarizado.\n",
    "    \n",
    "    Pero ejecutamos muchos proyectos, el equipo está creciendo y estamos escalando muy rápido.\n",
    "    \n",
    "    Por lo tanto, nos encontramos con muchos problemas. ¿Cómo se entrenó el modelo? ¿Con qué datos? ¿Qué parámetros se usaron para las diferentes versiones? ¿Cómo podemos reproducirlos?\n",
    "    \n",
    "    Sentimos la necesidad de controlar nuestros experimentos…\n",
    "\n",
    "La verdad es que, cuando se desarrollan modelos de Machine Learning (ML), se realizan muchos experimentos. \n",
    "\n",
    "Y esos experimentos pueden:\n",
    "\n",
    "+ Usar diferentes modelos y hiperparámetros.\n",
    "+ Emplear distintos datos de entrenamiento o evaluación.\n",
    "+ Ejecutar diferentes códigos (incluyendo ese pequeño cambio que se quiso probar el otro día).\n",
    "+ Correr el mismo código en un entorno diferente (sin saber qué versión de las librerías estaba instalada).\n",
    "\n",
    "Como resultado, cada uno de estos experimentos puede producir métricas de evaluación completamente diferentes. Cada ajuste en los parámetros o cambios en el algoritmo puede tener un gran impacto en el rendimiento del modelo, al igual que las variaciones en la receta afectan el sabor de las galletas.\n",
    "\n",
    "Mantener un seguimiento de toda esa información se vuelve realmente difícil muy rápidamente. Especialmente si se desea organizar y comparar muchos experimentos y sentirse seguro de haber seleccionado los mejores modelos para llevar a producción.\n",
    "\n",
    "Aquí es donde entra el seguimiento de experimentos. Llevar un registro detallado de los experimentos permite guardar información sobre qué cambios produjeron mejores resultados y cuáles no, facilitando la identificación del enfoque más efectivo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89eabca25e87783e",
   "metadata": {},
   "source": [
    "## 2. Definiciones\n",
    "\n",
    "* **Seguimiento de Experimentos** en el contexto del aprendizaje automático se refiere al proceso de registrar y organizar sistemáticamente toda la información relevante sobre cada experimento realizado durante el desarrollo de modelos de ML. Un experimento de ML es un enfoque sistemático para probar una hipótesis, y su metadata relevante incluye los insumos y salidas del experimento.\n",
    "\n",
    "* **Experiment**:\n",
    "   - **Definición**: Un experimento es un grupo lógico de ejecuciones (runs). Es un contenedor para rastrear diferentes versiones de modelos o configuraciones dentro de un proyecto de aprendizaje automático. Cada experimento puede contener varias ejecuciones con distintos parámetros, algoritmos o conjuntos de datos, lo que facilita la comparación entre versiones del modelo.\n",
    "   - **Ejemplo**: En un proyecto de clasificación de imágenes de gatos y perros, podrías tener un experimento llamado \"Clasificador de Gatos y Perros\". En este experimento, probarías diferentes modelos (`ResNet50`, `VGG16`, `MobileNet`) y ajustes de hiperparámetros, evaluando cuál ofrece el mejor rendimiento.\n",
    "\n",
    "* **Run**:\n",
    "   - **Definición**: Un run es la ejecución individual de un modelo de entrenamiento dentro de un experimento. Cada run registra detalles como parámetros (entradas), métricas (salidas), y otros metadatos importantes. Un nuevo run es creado cada vez que se ejecuta un trabajo de entrenamiento, y se le asigna un ID único para su seguimiento.\n",
    "   - **Ejemplo**: Ejecutar un entrenamiento con `ResNet50` y una tasa de aprendizaje de 0.001 y un batch size de 32 se registraría como un _run_. Este run podría incluir:\n",
    "     - **Modelo:** ResNet50\n",
    "     - **Parámetros:**\n",
    "       - `learning_rate = 0.001`\n",
    "       - `batch_size = 32`\n",
    "     - **Métricas:**\n",
    "       - `accuracy = 0.89`\n",
    "       - `loss = 0.35`\n",
    "     - **Artefactos:** El modelo entrenado, un gráfico de la matriz de confusión, y los logs del proceso de entrenamiento.\n",
    "\n",
    "* **Artifact**:\n",
    "   - **Definición**: Un artefacto se refiere a cualquier archivo o dato generado como parte de un experimento de ML. Esto puede incluir el modelo entrenado, resultados de evaluación, gráficos, archivos de predicción, o conjuntos de datos usados.\n",
    "   - **Ejemplo**: Después de un run, los artefactos generados podrían incluir:\n",
    "     - El modelo entrenado (e.g., `model_resnet50_v1.pkl`).\n",
    "     - Un gráfico de las curvas de aprendizaje (`learning_curves.png`).\n",
    "     - Un archivo CSV con las predicciones (`predictions.csv`).\n",
    "   \n",
    "* **Metadata**:\n",
    "   - **Definición**: Los metadatos son la información que describe las propiedades de un `experimento` o un `run`. Incluye los parámetros registrados, las métricas calculadas, las etiquetas asociadas, y detalles del entorno de entrenamiento, como las versiones de las librerías usadas. Los metadatos ayudan a organizar y entender los resultados, proporcionando contexto sobre las condiciones en las que se entrenaron los modelos.\n",
    "   - **Ejemplo**: Para un `run` que entrena un modelo `VGG16`, los metadatos podrían incluir:\n",
    "     - **Parámetros:**\n",
    "       - `learning_rate = 0.0001`\n",
    "       - `epochs = 20`\n",
    "       - `batch_size = 64`\n",
    "     - **Métricas:**\n",
    "       - `accuracy = 0.92`\n",
    "       - `precision = 0.91`\n",
    "     - **Etiquetas:**\n",
    "       - `experiment_type = hyperparameter_tuning`\n",
    "       - `model_type = CNN`\n",
    "     - **Entorno:** `Versión de Python 3.9, TensorFlow 2.6, CUDA 11.2.`\n",
    "\n",
    "### 2.1 Componentes Clave del Seguimiento de Experimentos\n",
    "\n",
    "1. **Hipótesis**: La suposición o prueba que se está evaluando. Por ejemplo, \"Si aumento el número de épocas, la precisión de validación aumentará\".\n",
    "\n",
    "2. **Insumos**:\n",
    "   - **Código**: Scripts y versiones del código utilizado para ejecutar el experimento.\n",
    "   - **Datos de Entrenamiento y Validación**: Conjuntos de datos utilizados, incluyendo características.\n",
    "   - **Arquitectura del Modelo e Hiperparámetros**: Configuración del modelo, como el tamaño de la red neuronal y el número de épocas.\n",
    "\n",
    "3. **Salidas**:\n",
    "   - **Métricas de Evaluación**: Precisión, recall, y otras métricas que indican el rendimiento del modelo.\n",
    "   - **Parámetros del Modelo**: Los parámetros finales del modelo que se han entrenado.\n",
    "\n",
    "### 2.2 Proceso y Propósito\n",
    "\n",
    "El desarrollo de un modelo de ML busca encontrar la mejor configuración del modelo en términos de métricas, uso de recursos o tiempo de inferencia, según las restricciones del proyecto. Este proceso iterativo implica ejecutar numerosos experimentos, analizar y comparar sus resultados, y probar nuevas ideas para desarrollar la configuración de mejor rendimiento. \n",
    "\n",
    "El seguimiento de experimentos permite:\n",
    "- **Registrar Parámetros**: Por ejemplo, tasa de aprendizaje, tamaño de lote.\n",
    "- **Rastrear Métricas**: Como precisión, recall.\n",
    "- **Almacenar Artefactos**: Archivos de modelos entrenados, gráficos.\n",
    "- **Guardar Metadatos**: Detalles del entorno, versiones de bibliotecas.\n",
    "\n",
    "Además, el seguimiento de experimentos facilita la comparación de modelos a lo largo del tiempo, la identificación de factores que afectan el rendimiento y la colaboración con colegas al compartir experimentos.\n",
    "\n",
    "### 2.3 Información Adicional\n",
    "\n",
    "Generalmente, el seguimiento de experimentos incluye:\n",
    "- Scripts utilizados para el experimento.\n",
    "- Archivos de configuración del entorno.\n",
    "- Información sobre los datos utilizados (por ejemplo, estadísticas y versiones de los conjuntos de datos).\n",
    "- Configuraciones del modelo y parámetros de entrenamiento.\n",
    "- Métricas de evaluación de ML.\n",
    "- Parámetros del modelo.\n",
    "- Visualizaciones de rendimiento (como matrices de confusión o curvas ROC).\n",
    "- Predicciones de ejemplo en el conjunto de validación (común en visión por computadora).\n",
    "\n",
    "El seguimiento de experimentos asegura que cada versión o configuración del modelo esté bien documentada y sea trazable, facilitando la colaboración y la gestión de modelos a lo largo de múltiples versiones. Idealmente, esta información debe estar disponible tanto durante como después de la ejecución del experimento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea81b455f38044b",
   "metadata": {},
   "source": [
    "## 3. ¿Lo Necesito?\n",
    "\n",
    "Un rastreador de experimentos permite reproducir cualquier modelo del pasado. Aunque no lo hace solo—para lograr una reproducibilidad completa también se necesita control de versiones de datos y control de versiones de código—un rastreador de experimentos es la única herramienta que combina toda la información relevante sobre un modelo.\n",
    "\n",
    "Cuando el rendimiento de un modelo cambia, los rastreadores de experimentos te permiten retroceder y entender por qué, lo que a su vez significa que puedes tomar las decisiones correctas para mejorar tu modelo en el futuro.\n",
    "\n",
    "Además, si tienes un experimento particular que deseas compartir con un colega para obtener su opinión o revisión, un rastreador de experimentos facilita que tu colega vea no solo el resultado final, sino exactamente cómo llegaste allí.\n",
    "\n",
    "Como herramienta tanto para garantizar la reproducibilidad como para habilitar la colaboración, el seguimiento de experimentos es una pieza clave de tu infraestructura de MLOps.\n",
    "\n",
    "### 3.1 ¿Por Qué Necesitas Rastrear Tus Experimentos de ML?\n",
    "\n",
    "Debido a que pequeños cambios en los insumos pueden llevar a resultados completamente diferentes, se realizarán muchos experimentos para desarrollar el mejor modelo. Sin registrar los insumos y salidas y organizar los experimentos, puedes perder rápidamente de vista lo que funcionó y lo que no.\n",
    "\n",
    "Por lo tanto, rastrear tus experimentos de ML de manera organizada puede ayudarte en los siguientes aspectos:\n",
    "\n",
    "- **Visión General**: ¿Cuántos y qué experimentos se realizaron?\n",
    "- **Detalles y Reproducibilidad**: ¿Cuáles fueron los detalles de los experimentos y cómo podemos reproducir los resultados?\n",
    "- **Comparación**: ¿Qué ideas y cambios llevaron a mejoras?\n",
    "\n",
    "Con la información obtenida, puedes centrarte en nuevos enfoques y en mejorar los prototipos en lugar de tratar de dar sentido a una gran cantidad de experimentos desorganizados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2d57ac59fe4313",
   "metadata": {},
   "source": [
    "## 4. ¿Cómo Rastrear Experimentos de Machine Learning?\n",
    "\n",
    "Se puede rastrear experimentos de machine learning de manera manual o automática utilizando diferentes herramientas. \n",
    "\n",
    "Se podría realizar el seguimiento manualmente con papel y bolígrafo o digitalmente en archivos de texto u hojas de cálculo. \n",
    "\n",
    "También puedes automatizar la tarea añadiendo funciones de registro a tu código o utilizando herramientas modernas de seguimiento de experimentos.\n",
    "\n",
    "Hay algunas opciones, siendo las más populares:\n",
    "\n",
    "* Hojas de cálculo y convención de nombres (Seguimiento Manual)\n",
    "* Versionando todo con un repositorio de Git\n",
    "* Uso moderno de herramientas para seguimiento de experimentos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc03d8359a988c1c",
   "metadata": {},
   "source": [
    "### 4.1 Seguimiento Manual de Experimentos (¡Por favor, NO!)\n",
    "\n",
    "Un enfoque común para el seguimiento de experimentos es crear una hoja de cálculo gigante donde se coloca toda la información posible (métricas, parámetros, etc.) y una estructura de directorios donde las cosas son nombradas de una manera específica. Esos nombres suelen terminar siendo muy largos e intrincados, como `model_v1_lr01_batchsize64_no_preprocessing_result_accuracy082.h5`.\n",
    "\n",
    "Cada vez que se ejecuta un experimento, los resultados son revisados y copiados en la hoja de cálculo.\n",
    "\n",
    "Este enfoque es sencillo y una buena manera de que los experimentos sean rastreados cuando se está comenzando.\n",
    "\n",
    "Sin embargo, se presentan varias desventajas:\n",
    "- **Disciplina y tiempo son requeridos**: Registrar manualmente todos los metadatos relevantes de un experimento requiere disciplina y tiempo.\n",
    "- **Errores son inevitablemente cometidos**: Los errores son inevitables durante el proceso de registro manual.\n",
    "- **Las notas se pueden perder**: Si las notas de los experimentos registradas manualmente se pierden (similar a no usar control de versiones y perder el código), muchos, si no todos, los experimentos podrían tener que ser ejecutados nuevamente.\n",
    "- **Escalabilidad es limitada**: Además, aunque esta tarea tediosa puede ser automatizada, el enfoque no escala bien cuando se necesitan ejecutar muchos experimentos.\n",
    "- **Debe garantizarse que ni tú ni tu equipo sobrescribirán accidentalmente la información en la hoja de cálculo**. Las hojas de cálculo no son fáciles de versionar, por lo que si esto sucede, estarás en problemas.\n",
    "- **Es necesario recordar rastrearlos**. Las cosas se complican si algo no sucede automáticamente, especialmente cuando hay más personas involucradas.\n",
    "- **Es necesario recordar utilizar las convenciones de nombres**. Si alguien en el equipo comete un error con esto, rastrear los artefactos del experimento (pesos del modelo, gráficos de rendimiento) será doloroso.\n",
    "- **Las carpetas de artefactos deben ser respaldadas independientemente y mantenidas en sincronía con la hoja de cálculo**. Incluso si un flujo de trabajo automático se configura para ejecutarse regularmente, inevitablemente llegará un momento en que falle.\n",
    "- **Cuando la hoja de cálculo crece, se vuelve menos usable**. Buscar y comparar cientos de experimentos en una hoja de cálculo (especialmente si varias personas quieren usarla al mismo tiempo) no es una gran experiencia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baf4026ba0f3256",
   "metadata": {},
   "source": [
    "### 4.2 Seguimiento Automático de Experimentos Sin Herramientas de Seguimiento de Experimentos\n",
    "\n",
    "Una forma popular de rastrear los experimentos de machine learning es automatizar el tedioso trabajo de registrar todo lo que podría ser importante, incluyendo funcionalidades de registro en tu código.\n",
    "\n",
    "Aunque este enfoque requiere un poco más de esfuerzo para configurarlo en comparación con el seguimiento manual de experimentos, es fácil de implementar, directo y te ahorra tiempo a largo plazo porque es menos propenso a errores (por ejemplo, cometer errores durante el registro manual, perder notas, etc.) en comparación con el seguimiento manual de experimentos.\n",
    "\n",
    "Veamos el flujo de trabajo aproximado del seguimiento automático de experimentos escribiendo código para registrar información en una hoja de cálculo.\n",
    "\n",
    "1. **Configuración**\n",
    "    Configura una hoja de cálculo. Existen varias formas diferentes de añadir funcionalidades de registro a tu código. En este ejemplo, leeremos la hoja de cálculo en un DataFrame de pandas y añadiremos una nueva fila para cada experimento.\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "    \n",
    "    log_df = pd.read_csv('log.csv')\n",
    "    ```\n",
    "\n",
    "2. **Registro de Insumos y Salidas**\n",
    "A continuación, registra toda la metadata relevante del experimento en un único diccionario. Luego, puedes añadir el diccionario del experimento al DataFrame de pandas como una nueva fila. Al final, puedes guardar el DataFrame de pandas de vuelta en la hoja de cálculo. También podrías automatizar el guardado de gráficos relevantes en una carpeta dedicada.\n",
    "    ```python\n",
    "    # Setup a new run\n",
    "    experiment = {'Experiment ID': 1}\n",
    "    \n",
    "    # Log inputs, such as hyperparameters\n",
    "    experiment['learning_rate'] = 1e-3\n",
    "    \n",
    "    # Model development here\n",
    "    # ...\n",
    "    \n",
    "    # Log outputs, such as metrics\n",
    "    experiment['val_acc'] = val_accuracy\n",
    "    \n",
    "    # ...\n",
    "    \n",
    "    # Save experiment details\n",
    "    log_df = log_df.append(experiment, ignore_index = True)\n",
    "    log_df.to_csv('log.csv', index = False)\n",
    "    ```\n",
    "\n",
    "3. **Recuperación de Información**\n",
    "   En tu hoja de cálculo, ahora puedes buscar, filtrar y ordenar los resultados de diferentes experimentos.\n",
    "    \n",
    "   <img style=\"display: block; margin: auto;\" src=\"./images/tracking-experiment-spreadsheet.png\" width=\"680\" height=\"50\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72f78c2d925e3e",
   "metadata": {},
   "source": [
    "### 4.3 Seguimiento Automatizado de Experimentos con Herramientas de Seguimiento de Experimentos\n",
    "\n",
    "Finalmente, existen herramientas modernas de seguimiento de experimentos, que son soluciones construidas específicamente para rastrear, organizar y comparar experimentos. \n",
    "\n",
    "Hay varias opciones populares, como:\n",
    "https://neptune.ai/blog/best-ml-experiment-tracking-tools\n",
    "\n",
    "\n",
    "* [MLFlow](https://mlflow.org/)\n",
    "* [CometML](https://www.comet.ml/)\n",
    "* [Neptune](https://neptune.ai/)\n",
    "* [Weights & Biases](https://wandb.ai/)\n",
    "* [TensorBoard](https://www.tensorflow.org/tensorboard)\n",
    "* [Otras](https://neptune.ai/blog/best-ml-experiment-tracking-tools):\n",
    "\n",
    "<img style=\"display: block; margin: auto;\" src=\"./images/tracking-experiment-tools.png\" width=\"680\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b23fe2379e7752",
   "metadata": {},
   "source": [
    "## 5. Mejores prácticas para el seguimiento de experimentos de ML\n",
    "\n",
    "Hasta ahora, se ha cubierto qué es el seguimiento de experimentos de aprendizaje automático y por qué es importante.\n",
    "\n",
    "Ahora es momento de entrar en detalles.\n",
    "\n",
    "### 5.1 Qué deberías rastrear en cualquier experimento de ML:\n",
    "\n",
    "Como se mencionó inicialmente, la información que se quiere rastrear depende en última instancia de las características del proyecto.\n",
    "\n",
    "Sin embargo, hay algunas cosas que deberías rastrear independientemente del proyecto en el que estés trabajando. Estas son:\n",
    "\n",
    "1. **Código**: Scripts de preprocesamiento, entrenamiento y evaluación, notebooks para ingeniería de características y otras utilidades. Y, por supuesto, todo el código necesario para ejecutar (y re-ejecutar) el experimento.\n",
    "\n",
    "2. **Entorno**: La forma más fácil de mantener un registro del entorno es guardar los archivos de configuración del entorno como `Dockerfile` (Docker), `requirements.txt` (pip), `pyproject.toml` (por ejemplo, `hatch` y `poetry`), o `conda.yml` (conda). También puedes guardar imágenes Docker construidas en Docker Hub o en tu propio repositorio de contenedores, pero encuentro más fácil guardar archivos de configuración.\n",
    "\n",
    "3. **Datos**: Guardar versiones de datos (como un hash o ubicaciones de recursos de datos inmutables) facilita ver en qué se entrenó tu modelo. También puedes usar herramientas modernas de versionado de datos como `DVC` (y guardar los archivos .dvc en tu herramienta de seguimiento de experimentos).\n",
    "\n",
    "4. **Parámetros**: Guardar la configuración de la ejecución de tu experimento es crucial. Ten especial cuidado cuando pases parámetros a través de la línea de comandos (por ejemplo, a través de argparse, click o hydra), ya que este es un lugar donde puedes olvidar fácilmente rastrear información importante. \n",
    "\n",
    "5. **Métricas**: Registrar métricas de evaluación en conjuntos de entrenamiento, validación y prueba para cada ejecución es bastante obvio. Pero diferentes frameworks lo hacen de manera diferente, así que tal vez quieras revisar este artículo en profundidad sobre el seguimiento de métricas de modelos de ML.\n",
    "\n",
    "Mantener un registro de estas cosas te permitirá reproducir experimentos, realizar depuraciones básicas y entender qué sucedió a un alto nivel.\n",
    "\n",
    "Dicho esto, siempre se pueden registrar más cosas para obtener aún más información. Mientras se mantengan los datos que se rastrean en una estructura agradable, no hace daño recopilar información, incluso si no se sabe si podría ser relevante más adelante. Después de todo, la mayoría de los metadatos son solo números y cadenas que no ocupan mucho espacio.\n",
    "\n",
    "### 5.2 Qué más podrías rastrear\n",
    "\n",
    "Veamos algunas cosas adicionales que se podrían querer rastrear al trabajar en un tipo específico de proyecto.\n",
    "\n",
    "A continuación se presentan algunas recomendaciones para varios tipos de proyectos de ML.\n",
    "\n",
    "#### 5.2.1 Aprendizaje Automático\n",
    "\n",
    "- Pesos del modelo\n",
    "- Gráficos de evaluación (curvas ROC, matriz de confusión)\n",
    "- Distribuciones de predicción\n",
    "\n",
    "#### 5.2.2 Aprendizaje Profundo\n",
    "\n",
    "- Puntos de control del modelo (tanto durante como después del entrenamiento)\n",
    "- Normas de gradiente (para controlar problemas de gradiente que desaparece o explota)\n",
    "- Mejores/peores predicciones en el conjunto de validación y prueba después del entrenamiento\n",
    "- Recursos de hardware: útil para depurar cargadores de datos y configuraciones multi-GPU\n",
    "\n",
    "#### 5.2.3 Visión por Computadora\n",
    "\n",
    "- Predicciones del modelo después de cada época (etiquetas, máscaras superpuestas o cuadros delimitadores)\n",
    "\n",
    "#### 5.2.4 Procesamiento del Lenguaje Natural y Modelos de Lenguaje Grande\n",
    "\n",
    "- Tiempo de inferencia\n",
    "- Prompts (en el caso de LLMs generativos)\n",
    "- Métricas de evaluación específicas (por ejemplo, ROUGE para resumen de texto o BLEU para traducción entre idiomas)\n",
    "- Tamaño y dimensiones de incrustación, tipo de tokenizador y número de cabezas de atención (al entrenar modelos de transformadores desde cero)\n",
    "- Importancia de características, explicaciones basadas en atención o basadas en ejemplos (ver esta visión general para algoritmos específicos y más ideas)\n",
    "\n",
    "#### 5.2.5 Datos Estructurados\n",
    "\n",
    "- Instantánea de datos de entrada (`.head()` en DataFrames si estás usando pandas)\n",
    "- Importancia de características (por ejemplo, importancia de permutación)\n",
    "- Explicaciones de predicción como SHAP o gráficos de dependencia parcial (todos están disponibles en DALEX)\n",
    "\n",
    "#### 5.2.6 Aprendizaje por Refuerzo\n",
    "\n",
    "- Retorno de episodio y duración de episodio\n",
    "- Pasos totales del entorno, tiempo de pared, pasos por segundo\n",
    "- Pérdidas de función de valor y política\n",
    "- Estadísticas agregadas sobre múltiples entornos y/o ejecuciones\n",
    "\n",
    "#### 5.2.7 Optimización de Hiperparámetros\n",
    "\n",
    "- Puntuación de ejecución: la métrica que estás optimizando después de cada iteración\n",
    "- Parámetros de ejecución: configuración de parámetros probada en cada iteración\n",
    "- Mejores parámetros: mejores parámetros hasta el momento y mejores parámetros generales después de que todas las ejecuciones hayan concluido\n",
    "- Gráficos de comparación de parámetros: hay varias visualizaciones que podrías querer registrar durante o después del entrenamiento, como gráfico de coordenadas paralelas o gráfico de corte (todos están disponibles en Optuna, por cierto)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53651998fbe8b31",
   "metadata": {},
   "source": [
    "## 6. MLflow\n",
    "\n",
    "<img style=\"display: block; margin: auto;\" src=\"./images/mlflow-logo.png\" width=\"480\" height=\"500\">\n",
    "\n",
    "### 6.1 Introducción:\n",
    "`MLflow` es una plataforma de código abierto indispensable para gestionar el ciclo de vida del aprendizaje automático. Aborda aspectos clave del proceso de aprendizaje automático, incluyendo la experimentación, la reproducibilidad, la implementación y el registro central de modelos.\n",
    "\n",
    "En la práctica, es solamente un paquete de `Python` que puede ser instalado con `pip`, y contiene 3 módulos principales:\n",
    "\n",
    "+ Tracking\n",
    "+ Model Registry\n",
    "+ Projects\n",
    "\n",
    "### 6.2 ¿Por qué usar `MLflow`?\n",
    "\n",
    "El proceso de aprendizaje automático (ML) es complejo, abarcando diversas etapas, desde el preprocesamiento de datos hasta el despliegue del modelo y su monitoreo. Asegurar la productividad y eficiencia a lo largo de este ciclo de vida plantea varios desafíos:\n",
    "\n",
    "- **Gestión de Experimentos**: Es difícil llevar un registro de la miríada de experimentos, especialmente cuando se trabaja con archivos o cuadernos interactivos. Determinar qué combinación de datos, código y parámetros condujo a un resultado particular puede convertirse en una tarea desalentadora.\n",
    "\n",
    "- **Reproducibilidad**: Asegurar resultados consistentes en diferentes ejecuciones no es trivial. Más allá de solo rastrear versiones de código y parámetros, capturar todo el entorno, incluyendo las dependencias de bibliotecas, es crítico. Esto se vuelve aún más desafiante al colaborar con otros científicos de datos o al escalar el código a diferentes plataformas.\n",
    "\n",
    "- **Consistencia en el Despliegue**: Con la plétora de bibliotecas de ML disponibles, a menudo no hay una forma estandarizada de empaquetar y desplegar modelos. Las soluciones personalizadas pueden llevar a inconsistencias, y el vínculo crucial entre un modelo y el código y parámetros que lo produjeron podría perderse.\n",
    "\n",
    "- **Gestión de Modelos**: A medida que los equipos de ciencia de datos producen numerosos modelos, gestionar, probar y desplegar continuamente estos modelos se convierte en un obstáculo significativo. Sin una plataforma centralizada, gestionar los ciclos de vida de los modelos se vuelve inmanejable.\n",
    "\n",
    "- **Agnosticismo de Biblioteca**: Aunque las bibliotecas individuales de ML podrían ofrecer soluciones a algunos de los desafíos, lograr los mejores resultados a menudo implica experimentar a través de múltiples bibliotecas. Una plataforma que ofrezca compatibilidad con varias bibliotecas mientras asegura que los modelos sean utilizables como \"cajas negras\" reproducibles es esencial.\n",
    "\n",
    "MLflow aborda estos desafíos ofreciendo una plataforma unificada diseñada para todo el ciclo de vida del ML. Sus beneficios incluyen:\n",
    "\n",
    "- **Trazabilidad**: Con herramientas como el Servidor de Seguimiento, cada experimento se registra, asegurando que los equipos puedan rastrear y entender la evolución de los modelos.\n",
    "\n",
    "- **Consistencia**: Ya sea accediendo a modelos a través de las Implementaciones de MLflow para LLMs o estructurando proyectos con Recetas de `MLflow`, `MLflow` promueve un enfoque consistente, reduciendo tanto la curva de aprendizaje como los posibles errores.\n",
    "\n",
    "- **Flexibilidad**: El diseño agnóstico de bibliotecas de `MLflow` asegura compatibilidad con una amplia gama de bibliotecas de machine learning. Ofrece soporte integral a través de diferentes lenguajes de programación, respaldado por una robusta API REST, CLI y APIs para Python, R y Java.\n",
    "\n",
    "Al simplificar el complejo paisaje de los flujos de trabajo de ML, `MLflow` empodera a los científicos de datos y desarrolladores a centrarse en construir y refinar modelos, asegurando un camino ágil desde la experimentación hasta la producción.\n",
    "\n",
    "###  6.3 ¿Quién usa MLFlow?\n",
    "![](https://www.mlflow.org/docs/latest/_images/mlflow-overview.png)\n",
    "\n",
    "Los Científicos de Datos aprovechan `MLflow` para:\n",
    "\n",
    "- Seguimiento de experimentos y persistencia de pruebas de hipótesis.\n",
    "- Estructuración de código para una mejor reproducibilidad.\n",
    "- Empaquetado de modelos y gestión de dependencias.\n",
    "- Evaluar los límites de selección de ajuste de hiperparámetros.\n",
    "- Comparar los resultados de re-entrenamiento de modelos a lo largo del tiempo.\n",
    "- Revisar y seleccionar modelos óptimos para el despliegue.\n",
    "\n",
    "Los Profesionales de MLOps utilizan `MLflow` para:\n",
    "\n",
    "- Gestionar los ciclos de vida de los modelos entrenados, tanto antes como después del despliegue.\n",
    "- Desplegar modelos de forma segura en entornos de producción.\n",
    "- Auditar y revisar modelos candidatos antes del despliegue.\n",
    "- Gestionar las dependencias de despliegue.\n",
    "\n",
    "Los Gerentes de Ciencia de Datos interactúan con `MLflow` mediante:\n",
    "\n",
    "- Revisión de los resultados de la experimentación y las actividades de modelado.\n",
    "- Colaboración con equipos para asegurar que los objetivos de modelado se alineen con los objetivos empresariales.\n",
    "\n",
    "Los Usuarios de Ingeniería de Prompts usan `MLflow` para:\n",
    "\n",
    "- Evaluar y experimentar con modelos de lenguaje de gran escala.\n",
    "- Crear prompts personalizados y persistir sus creaciones candidatas.\n",
    "- Decidir sobre el mejor modelo base adecuado para los requisitos específicos de su proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39eee0efe2983ce5",
   "metadata": {},
   "source": [
    "### 6.4 `MLflow` Tracking\n",
    "\n",
    "`MLflow Tracking` es uno de los componentes principales de servicio de `MLflow`\n",
    "\n",
    "<img style=\"display: block; margin: auto;\" src=\"./images/mlflow-tracking-basics.png\" width=\"680\" height=\"500\">\n",
    "\n",
    "\n",
    "<img style=\"display: block; margin: auto;\" src=\"./images/mlflow-run-comparison.png\" width=\"680\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a4fa2607fa5f1f",
   "metadata": {},
   "source": [
    "## 7. Hands-On - `MLflow` Tracking\n",
    "https://mlflow.org/docs/latest/tracking/tracking-api.html\n",
    "\n",
    "Primero, vamos a explorar y aprender cómo hacer seguimiento de experimentos de una manera sencilla, luego revisaremos un ejemplo un poco más avanzado\n",
    "\n",
    "### 7.1 Setup\n",
    "\n",
    "Vamos a retomar el ejemplo de la clase pasada, pero primero vamos a asegurarnos de tener las librerías necesarias instaladas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27754f979706ada",
   "metadata": {},
   "source": [
    "```bash\n",
    "pip install mlflow scikit-learn pandas seaborn hyperopt xgboost fastparquet boto3 pyarrow\n",
    "```\n",
    "\n",
    "Ahora vamos a ejecutar el `tracking server` con el siguiente comando y explorar un poco lo que vamos a encontrar\n",
    "```bash\n",
    "cd Módulo\\ 3/ \n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db\n",
    "```\n",
    "   "
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Creamos una carpeta `data` para descargar los datos necesarios.",
   "id": "cd7395b5d6cb01f5"
  },
  {
   "cell_type": "code",
   "id": "65472cd4395bded3",
   "metadata": {},
   "source": [
    "# Create the directory if it doesn't exist\n",
    "!mkdir -p ./data\n",
    "\n",
    "# Download files using curl\n",
    "!curl -o ./data/green_tripdata_2024-01.parquet https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-01.parquet\n",
    "!curl -o ./data/green_tripdata_2024-02.parquet https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2024-02.parquet"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Importar las librerías necesarias y definir función para importar los datos",
   "id": "c149530b1c2a692f"
  },
  {
   "cell_type": "code",
   "id": "71acc1b539477c9a",
   "metadata": {},
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.metrics import  root_mean_squared_error\n",
    "from sklearn.feature_extraction import  DictVectorizer\n",
    "from sklearn.linear_model import Lasso, Ridge, LinearRegression"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a60cce3f565662de",
   "metadata": {},
   "source": [
    "def read_dataframe(filename):\n",
    "\n",
    "    df = pd.read_parquet(filename)\n",
    "\n",
    "    df['duration'] = df.lpep_dropoff_datetime - df.lpep_pickup_datetime\n",
    "    df.duration = df.duration.apply(lambda td: td.total_seconds() / 60)\n",
    "\n",
    "    df = df[(df.duration >= 1) & (df.duration <= 60)]\n",
    "\n",
    "    categorical = ['PULocationID', 'DOLocationID']\n",
    "    df[categorical] = df[categorical].astype(str)\n",
    "\n",
    "    return df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b3a6b3a6f3a0309d",
   "metadata": {},
   "source": [
    "df_train = read_dataframe('./data/green_tripdata_2024-01.parquet')\n",
    "df_val = read_dataframe('./data/green_tripdata_2024-02.parquet')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Feature Engineering",
   "id": "2f35d1469567193"
  },
  {
   "cell_type": "code",
   "id": "fab9b1bb8c74459b",
   "metadata": {},
   "source": [
    "df_train['PU_DO'] = df_train['PULocationID'] + '_' + df_train['DOLocationID']\n",
    "df_val['PU_DO'] = df_val['PULocationID'] + '_' + df_val['DOLocationID']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "One Hot Encoding",
   "id": "dd914dab8bf2abf6"
  },
  {
   "cell_type": "code",
   "id": "d75d7c178ccf5d02",
   "metadata": {},
   "source": [
    "categorical = ['PU_DO']  #'PULocationID', 'DOLocationID']\n",
    "numerical = ['trip_distance']\n",
    "dv = DictVectorizer()\n",
    "\n",
    "train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "X_train = dv.fit_transform(train_dicts)\n",
    "\n",
    "val_dicts = df_val[categorical + numerical].to_dict(orient='records')\n",
    "X_val = dv.transform(val_dicts)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d354960756ea3aeb",
   "metadata": {},
   "source": [
    "target = 'duration'\n",
    "y_train = df_train[target].values\n",
    "y_val = df_val[target].values"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.2 Ejemplo sencillo",
   "id": "9cbee4ef18f331db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definir el `tracking URI` y el nombre del experimento",
   "id": "eca87c6aae58b139"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import mlflow\n",
    "\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(experiment_name=\"class-examples-nyc-taxi-experiment\")"
   ],
   "id": "f22a3fa6bf063568",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definir los `dataset` como objetos de `mlflow` para poderlos trackear",
   "id": "2f223a38aad102c5"
  },
  {
   "cell_type": "code",
   "id": "d65e3590beb28271",
   "metadata": {},
   "source": [
    "training_dataset = mlflow.data.from_numpy(X_train.data, targets=y_train, name=\"green_tripdata_2024-01\")\n",
    "validation_dataset = mlflow.data.from_numpy(X_val.data, targets=y_val, name=\"green_tripdata_2024-02\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with mlflow.start_run(run_name=\"Lasso\"): \n",
    "    \n",
    "    # Set a custom tag\n",
    "\n",
    "    \n",
    "    # Log inputs with context\n",
    "\n",
    "\n",
    "    \n",
    "    # Hyper-parameter\n",
    "    alpha = 0.1\n",
    "    \n",
    "    # Log param\n",
    "    \n",
    "    \n",
    "    # Define model and train it\n",
    "    lr = Lasso(alpha=alpha)\n",
    "    lr.fit(X_train, y_train)\n",
    "    \n",
    "    # Log sklearn model with artifact_path\n",
    "\n",
    "\n",
    "    # Make predictions on validation set\n",
    "    y_pred = lr.predict(X_val)\n",
    "    \n",
    "    # Calculate performance metric\n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "    \n",
    "    # Log metric\n"
   ],
   "id": "a4783ec6218cf407",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7.3 Actividad en clase\n",
    "\n",
    "Probar los mismos modelos que ejecutamos la clase pasada, pero ahora logueado los datos con `mlflow`"
   ],
   "id": "ccb9a516debb35ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "lr = Lasso(0.01)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "root_mean_squared_error(y_val, y_pred)"
   ],
   "id": "914444b2fd02f8ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5e3366e3dcb1e132",
   "metadata": {},
   "source": [
    "lr = Lasso(0.001)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "root_mean_squared_error(y_val, y_pred)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "caba7f63fc8865",
   "metadata": {},
   "source": [
    "lr = Ridge(0.1)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "root_mean_squared_error(y_val, y_pred)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b5a9e6f8b9411b5a",
   "metadata": {},
   "source": [
    "lr = Ridge(0.01)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "root_mean_squared_error(y_val, y_pred)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "30b82c28c8c1462e",
   "metadata": {},
   "source": [
    "lr = Ridge(0.001)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lr.predict(X_val)\n",
    "\n",
    "root_mean_squared_error(y_val, y_pred)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9001801d1f4214a5",
   "metadata": {},
   "source": [
    "### 7.4 ¿Qué frameworks puedo loguear con `MLflow`?\n",
    "\n",
    "https://mlflow.org/docs/latest/models.html#built-in-model-flavors"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7.5 Ejemplo con tunning de hyper-parámetros\n",
    "\n",
    "Ahora vamos a entrenar un modelo `xgboost`\n"
   ],
   "id": "1339b6036e1f67fc"
  },
  {
   "cell_type": "code",
   "id": "bc0e77baa05ffae2",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from hyperopt.pyll import scope"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Si sale un error con la librería `libomp`, hay que instarla:\n",
    "\n",
    "Linux\n",
    "```bash\n",
    "apt-get install libgomp1\n",
    "```\n",
    "\n",
    "MacOS: \n",
    "```bash\n",
    "brew install libomp\n",
    "```\n",
    "\n",
    "Windows:\n",
    "https://stackoverflow.com/a/76327163"
   ],
   "id": "1477dbb7be2523f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definir los `dataset` a trabajar.",
   "id": "e50e426fb2200863"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train = xgb.DMatrix(X_train, label=y_train)\n",
    "valid = xgb.DMatrix(X_val, label=y_val)"
   ],
   "id": "c3e7f14256dbee8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definir la función objetivo",
   "id": "546e530983906190"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def objective(params):\n",
    "    with mlflow.start_run(run_name=\"xgboost-hyper-opt\"):\n",
    "        \n",
    "        # Log inputs with context\n",
    "        mlflow.log_input(training_dataset, context=\"training\")\n",
    "        mlflow.log_input(validation_dataset, context=\"validation\")\n",
    "         \n",
    "        # Tag model\n",
    "        mlflow.set_tag(\"model\", \"xgboost\")\n",
    "        \n",
    "        # Log parameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Train model\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=train,\n",
    "            num_boost_round=100,\n",
    "            evals=[(valid, 'validation')],\n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "        \n",
    "        # Log xgboost model with artifact_path\n",
    "        mlflow.xgboost.log_model(booster, artifact_path=\"model\")\n",
    "         \n",
    "        # Predict in the val dataset\n",
    "        y_pred = booster.predict(valid)\n",
    "        \n",
    "        # Calculate metric\n",
    "        rmse = root_mean_squared_error(y_val, y_pred)\n",
    "        \n",
    "        # Log performance metric\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    return {'loss': rmse, 'status': STATUS_OK}"
   ],
   "id": "6c568b0ecc221c4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definir el espacio de búsqueda",
   "id": "79cc38626d013606"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "search_space = {\n",
    "    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "    'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "    'objective': 'reg:squarederror',\n",
    "    'seed': 42\n",
    "}"
   ],
   "id": "19259e359f8b8ba3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Minimizar función objetivo",
   "id": "f73ebbafc4630b5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_result = fmin(\n",
    "    fn=objective,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=30,\n",
    "    trials=Trials()\n",
    ")"
   ],
   "id": "670dd19624ace652",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "best_result",
   "id": "c437008b0ca171fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.6 Autologging",
   "id": "798ed2a22e965d30"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Vamos a entrenar de nuevo el modelo con los mejores parámetros\n",
    "\n",
    "Librerías soportadas con autologging:\n",
    "\n",
    "https://mlflow.org/docs/latest/tracking/autolog.html#supported-libraries"
   ],
   "id": "54d3b354597fc339"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "params = best_result\n",
    "\n",
    "params[\"max_depth\"] = 39\n",
    "\n",
    "params[\"seed\"] = 42\n",
    "\n",
    "params[\"objective\"] = \"reg:squarederror\"\n",
    "\n",
    "params"
   ],
   "id": "2be61e7d07d8e579",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "mlflow.xgboost.autolog(disable=False)",
   "id": "871b5803bfef13e7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with mlflow.start_run(run_name=\"Xgboost Best Params Autologging\"):\n",
    "\n",
    "    booster = xgb.train(\n",
    "        params=params,\n",
    "        dtrain=train,\n",
    "        num_boost_round=100,\n",
    "        evals=[(valid, 'validation')],\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "\n",
    "    y_pred = booster.predict(valid)\n",
    "    \n",
    "    rmse = root_mean_squared_error(y_val, y_pred)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    \n",
    "    !mkdir models\n",
    "    with open(\"models/preprocessor.b\", \"wb\") as f_out:\n",
    "        pickle.dump(dv, f_out)\n",
    "        \n",
    "    mlflow.log_artifact(\"models/preprocessor.b\", artifact_path=\"preprocessor\")"
   ],
   "id": "97b6ad07b1909a30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 7.7 Usando los modelos loggueados",
   "id": "5df220309ca3e64b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "logged_model = \"\"\n",
    "\n",
    "# Load model as a PyFuncModel.\n",
    "loaded_model = mlflow.pyfunc.load_model(logged_model)\n",
    "\n",
    "# Load model as a XGBoost model.\n",
    "xgboost_model = mlflow.xgboost.load_model(logged_model)"
   ],
   "id": "dc7f34dd7f0e2a2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_predict_funct = loaded_model.predict(X_val)\n",
    "\n",
    "y_predict_funct"
   ],
   "id": "72d696495aa1600b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_predict = xgboost_model.predict(valid)\n",
    "\n",
    "y_predict"
   ],
   "id": "dfaeeae59f71bd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7.8 Tips para hacer Tracking con `MLflow`\n",
    "\n",
    "https://mlflow.org/docs/latest/tracking/tracking-api.html#tracking-tips"
   ],
   "id": "711c389373dbc04d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
